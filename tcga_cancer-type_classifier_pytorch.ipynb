{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "#from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 288.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate integer dimensions for numpy \"image\" matrix\n",
    "# used to represent gene expression data from text file\n",
    "# NOTE: the last 3 gene expression values are dropped from\n",
    "# each sample to achieve an integer dimension for the matrix\n",
    "# TODO: add 0-padding to avoid dropping the last 3 values\n",
    "height = 225\n",
    "width = 64800 / height\n",
    "print(height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version takes the input dims as an arg\n",
    "# and calculates the total features output from the last conv2d\n",
    "# layer for sizing the first fully-connected layer\n",
    "class MyConv2DNet(nn.Module):\n",
    "    def __init__(self, inshape=(1,224,270)):\n",
    "        super(MyConv2DNet, self).__init__()\n",
    "        # 1 input channel for FPKM values, 12 filters, 3x3 conv\n",
    "        self.conv1 = nn.Conv2d(1, 12, 3, padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(2, 2) \n",
    "        self.conv2 = nn.Conv2d(12, 32, 3, padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=(1,1))\n",
    "        self.convdrop1 = nn.Dropout2d(p=0.3)\n",
    "        \n",
    "        conv_out_feats = self._get_conv_outshape(inshape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_out_feats, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 13) # 13 classes for cancer types represented in train/valid data\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.softmax(self.fc3(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dims except batch dim\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def _conv_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv_outshape(self, inshape):\n",
    "        batchsz = 1\n",
    "        dummy_input = Variable(torch.rand(batchsz, *inshape))\n",
    "        dummy_output = self._conv_forward(dummy_input)\n",
    "        feature_cnt = dummy_output.data.view(batchsz, -1).size(1)\n",
    "        return feature_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class for the gene-fpkm values data (based on pytorch data processing tutorial:\n",
    "# pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "\n",
    "# This version normalizes fpkm values from 0 to 255 (for single channel \"image\" representation)\n",
    "\n",
    "class GeneExpressionFPKMDatasetN1(Dataset):\n",
    "    \"\"\"Gene expression RNAseq FPKM values dataset.\"\"\"\n",
    "    def __init__(self, samplenames_file, data_dir, transform=None):\n",
    "        self.filename_frame = pd.read_csv((os.path.join(data_dir, samplenames_file)),\n",
    "                                           sep=\"\\s+\", names=[\"filename\",\"label\"],\n",
    "                                          dtype={'filename':'object', 'label':'category'})\n",
    "        self.root_dir = data_dir\n",
    "        # 180518 store original class names from manifest file\n",
    "        self.class_encoder = LabelEncoder().fit(self.filename_frame.label)\n",
    "        #self.labels = self.labelstrs.factorize(sort=True)[0]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filename_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_name = os.path.join(self.root_dir, self.filename_frame.iloc[idx, 0])\n",
    "        label = self.class_encoder.transform([self.filename_frame.iloc[idx, 1]])\n",
    "        #label = np.array([self.labels[idx]])\n",
    "        pxdf = pd.read_csv(sample_name, sep='\\s+', header=None, names=['fpkm'])\n",
    "        #pxdf['normval'] = pxdf.fpkm.apply(lambda x: floor(255 if x == 1.0 else x * 256.0))\n",
    "        pxdf['normval'] = (pxdf.fpkm - pxdf.fpkm.mean()) / pxdf.fpkm.std() \n",
    "        fpkmvals = pxdf.normval.astype(np.float32).values\n",
    "        fpkmvals = fpkmvals[3:] # drop first 3 vals to keep shape\n",
    "        \n",
    "        # NOTE: no transforms for now (values are not loaded with torchvision)\n",
    "        # torch image: C X H X W\n",
    "        fpkmvals = fpkmvals.reshape((1, 224, 270))\n",
    "        fpkmvals = torch.from_numpy(fpkmvals)\n",
    "        label = torch.from_numpy(label)\n",
    "        \n",
    "        return {'fpkmvals': fpkmvals.type(torch.FloatTensor),\n",
    "                'label': label.type(torch.LongTensor)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle manifest file to try to ensure both classes represented in any given training batch\n",
    "testdata = GeneExpressionFPKMDatasetN1(\"test_manifest.txt\", \"/ssdata/data/tcga/fpkm13class/test/\")\n",
    "traindata = GeneExpressionFPKMDatasetN1(\"train_manifest.txt\", \"/ssdata/data/tcga/fpkm13class/train/\")\n",
    "validdata = GeneExpressionFPKMDatasetN1(\"valid_manifest.txt\", \"/ssdata/data/tcga/fpkm13class/valid/\")\n",
    "\n",
    "testloader = DataLoader(testdata, batch_size=8, shuffle=True, num_workers=4)\n",
    "trainloader = DataLoader(traindata, batch_size=8, shuffle=True, num_workers=4)\n",
    "validloader = DataLoader(validdata, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper functions for net initialization, training runs, and quick accuracy check\n",
    "# TODO: create cross-validation procedure and work on visualization of loss during training\n",
    "\n",
    "def newNetCuda(model):\n",
    "    net = model()\n",
    "    net.training = True\n",
    "    net.cuda()\n",
    "    net.zero_grad()\n",
    "    return net\n",
    "\n",
    "    \n",
    "def trainRun(model, epochs=10, lr=1.0e-3):\n",
    "    net.training = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, eps=1e-6)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs = data['fpkmvals']\n",
    "            labels = data['label']\n",
    "            inputs, labels = Variable(inputs.cuda(0)), Variable(labels.cuda(0))\n",
    "            labels = labels.squeeze() # must be Tensor of dim BATCH_SIZE, MUST BE 1Dimensional!\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # running stats\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 8 == 7:\n",
    "                print(\"[%d, %5d] loss: %.4f\" %\n",
    "                      (epoch + 1, i + 1, running_loss / 8))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "    print(\"Finished\")\n",
    "    \n",
    "def checkAccuracy(model, tloader, vloader):\n",
    "    net = model\n",
    "    net.training = False\n",
    "    trainloader = tloader\n",
    "    validloader = vloader\n",
    "    class_correctT = list(0. for i in range(13))\n",
    "    class_totalT = list(0. for i in range(13))\n",
    "    classes = (\"BRCA\",\"COAD\",\"GBM\",\"HNSC\",\"KIRC\",\"KIRP\",\"LGG\",\"LUAD\",\"LUSC\",\n",
    "               \"PRAD\",\"READ\",\"THCA\",\"UCEC\")\n",
    "    for data in trainloader:\n",
    "        imagesT, labelsT = data['fpkmvals'], data['label']\n",
    "        outputsT = net(Variable(imagesT).cuda())\n",
    "        _, predicted = torch.max(outputsT.data, 1)\n",
    "        c = (predicted.cpu() == labelsT.squeeze()).numpy()\n",
    "        for i in range(len(labelsT)):\n",
    "            label = int(labelsT[i][0])\n",
    "            class_correctT[label] += c[i]\n",
    "            class_totalT[label] += 1\n",
    "    print(\"Training set accuracy: \")    \n",
    "    for i in range(len(classes)):\n",
    "        print(\"Accuracy of %5s: %2d%%\" % (classes[i], 100 * class_correctT[i] / class_totalT[i]))\n",
    "    \n",
    "    class_correctV = list(0. for i in range(13))\n",
    "    class_totalV = list(0. for i in range(13))\n",
    "    for data in validloader:\n",
    "        imagesV, labelsV = data['fpkmvals'], data['label']\n",
    "        outputsV = net(Variable(imagesV).cuda())\n",
    "        _, predicted = torch.max(outputsV.data, 1)\n",
    "        c = (predicted.cpu() == labelsV.squeeze()).numpy()\n",
    "        for i in range(len(labelsV)):\n",
    "            label = int(labelsV[i][0])\n",
    "            class_correctV[label] += c[i]\n",
    "            class_totalV[label] += 1\n",
    "    print(\"Validation set accuracy: \") \n",
    "    for i in range(len(classes)):\n",
    "        print(\"Accuracy of %5s: %2d%%\" % (classes[i], 100 * class_correctV[i] / class_totalV[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     8] loss: 3.2833\n",
      "[1,    16] loss: 2.4017\n",
      "[1,    24] loss: 2.3748\n",
      "[1,    32] loss: 2.3556\n",
      "[1,    40] loss: 2.3391\n",
      "[1,    48] loss: 2.2472\n",
      "[1,    56] loss: 2.2995\n",
      "[1,    64] loss: 2.2185\n",
      "[1,    72] loss: 1.6702\n",
      "[1,    80] loss: 2.0554\n",
      "[1,    88] loss: 1.7738\n",
      "[1,    96] loss: 1.7982\n",
      "[1,   104] loss: 1.6054\n",
      "[1,   112] loss: 1.5287\n",
      "[1,   120] loss: 1.8138\n",
      "[1,   128] loss: 1.8698\n",
      "[1,   136] loss: 1.2341\n",
      "[1,   144] loss: 1.5530\n",
      "[1,   152] loss: 1.5774\n",
      "[1,   160] loss: 1.3712\n",
      "[1,   168] loss: 1.4780\n",
      "[1,   176] loss: 1.2406\n",
      "[1,   184] loss: 1.0653\n",
      "[1,   192] loss: 1.4540\n",
      "[1,   200] loss: 1.2252\n",
      "[1,   208] loss: 1.0775\n",
      "[1,   216] loss: 1.3754\n",
      "[1,   224] loss: 0.8953\n",
      "[1,   232] loss: 1.5754\n",
      "[1,   240] loss: 1.2022\n",
      "[1,   248] loss: 1.0070\n",
      "[1,   256] loss: 1.0558\n",
      "[1,   264] loss: 1.1291\n",
      "[1,   272] loss: 0.9117\n",
      "[1,   280] loss: 0.9309\n",
      "[1,   288] loss: 1.1729\n",
      "[1,   296] loss: 1.1357\n",
      "[1,   304] loss: 1.1468\n",
      "[1,   312] loss: 1.1349\n",
      "[1,   320] loss: 1.1331\n",
      "[1,   328] loss: 1.0935\n",
      "[1,   336] loss: 0.8810\n",
      "[1,   344] loss: 0.9433\n",
      "[1,   352] loss: 0.8509\n",
      "[1,   360] loss: 0.7454\n",
      "[1,   368] loss: 0.8559\n",
      "[1,   376] loss: 0.8001\n",
      "[1,   384] loss: 0.8082\n",
      "[1,   392] loss: 0.6029\n",
      "[1,   400] loss: 0.7530\n",
      "[1,   408] loss: 0.8082\n",
      "[1,   416] loss: 0.8997\n",
      "[1,   424] loss: 0.8434\n",
      "[1,   432] loss: 0.6460\n",
      "[1,   440] loss: 0.7529\n",
      "[1,   448] loss: 0.8054\n",
      "[1,   456] loss: 0.7225\n",
      "[1,   464] loss: 0.7656\n",
      "[1,   472] loss: 0.6834\n",
      "[2,     8] loss: 0.9845\n",
      "[2,    16] loss: 0.5080\n",
      "[2,    24] loss: 0.7917\n",
      "[2,    32] loss: 0.7267\n",
      "[2,    40] loss: 0.7769\n",
      "[2,    48] loss: 0.8881\n",
      "[2,    56] loss: 0.7854\n",
      "[2,    64] loss: 0.6818\n",
      "[2,    72] loss: 0.5275\n",
      "[2,    80] loss: 0.7604\n",
      "[2,    88] loss: 0.5458\n",
      "[2,    96] loss: 0.6062\n",
      "[2,   104] loss: 0.7149\n",
      "[2,   112] loss: 0.6639\n",
      "[2,   120] loss: 0.4559\n",
      "[2,   128] loss: 0.5303\n",
      "[2,   136] loss: 0.7378\n",
      "[2,   144] loss: 0.4767\n",
      "[2,   152] loss: 0.6221\n",
      "[2,   160] loss: 0.7983\n",
      "[2,   168] loss: 0.6340\n",
      "[2,   176] loss: 0.5888\n",
      "[2,   184] loss: 0.4358\n",
      "[2,   192] loss: 0.5193\n",
      "[2,   200] loss: 0.5877\n",
      "[2,   208] loss: 0.3948\n",
      "[2,   216] loss: 0.6831\n",
      "[2,   224] loss: 0.6118\n",
      "[2,   232] loss: 0.7206\n",
      "[2,   240] loss: 0.6973\n",
      "[2,   248] loss: 0.3885\n",
      "[2,   256] loss: 0.4749\n",
      "[2,   264] loss: 0.5644\n",
      "[2,   272] loss: 0.6393\n",
      "[2,   280] loss: 0.4834\n",
      "[2,   288] loss: 0.5382\n",
      "[2,   296] loss: 0.7433\n",
      "[2,   304] loss: 0.6009\n",
      "[2,   312] loss: 0.6613\n",
      "[2,   320] loss: 0.6459\n",
      "[2,   328] loss: 0.5520\n",
      "[2,   336] loss: 0.5621\n",
      "[2,   344] loss: 0.7436\n",
      "[2,   352] loss: 0.4749\n",
      "[2,   360] loss: 0.7907\n",
      "[2,   368] loss: 0.5367\n",
      "[2,   376] loss: 0.4175\n",
      "[2,   384] loss: 0.7592\n",
      "[2,   392] loss: 0.4174\n",
      "[2,   400] loss: 0.7602\n",
      "[2,   408] loss: 0.6444\n",
      "[2,   416] loss: 0.6836\n",
      "[2,   424] loss: 0.6936\n",
      "[2,   432] loss: 0.4862\n",
      "[2,   440] loss: 0.6809\n",
      "[2,   448] loss: 0.5938\n",
      "[2,   456] loss: 0.4230\n",
      "[2,   464] loss: 0.3143\n",
      "[2,   472] loss: 0.5976\n",
      "Finished\n",
      "CPU times: user 21.8 s, sys: 8.45 s, total: 30.2 s\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "net = newNetCuda(model=MyConv2DNet)\n",
    "%time trainRun(net, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: \n",
      "Accuracy of  BRCA: 99%\n",
      "Accuracy of  COAD: 98%\n",
      "Accuracy of   GBM: 81%\n",
      "Accuracy of  HNSC: 94%\n",
      "Accuracy of  KIRC: 85%\n",
      "Accuracy of  KIRP: 94%\n",
      "Accuracy of   LGG: 98%\n",
      "Accuracy of  LUAD: 83%\n",
      "Accuracy of  LUSC: 89%\n",
      "Accuracy of  PRAD: 98%\n",
      "Accuracy of  READ:  0%\n",
      "Accuracy of  THCA: 99%\n",
      "Accuracy of  UCEC: 93%\n",
      "Validation set accuracy: \n",
      "Accuracy of  BRCA: 98%\n",
      "Accuracy of  COAD: 100%\n",
      "Accuracy of   GBM: 100%\n",
      "Accuracy of  HNSC: 90%\n",
      "Accuracy of  KIRC: 85%\n",
      "Accuracy of  KIRP: 95%\n",
      "Accuracy of   LGG: 93%\n",
      "Accuracy of  LUAD: 67%\n",
      "Accuracy of  LUSC: 78%\n",
      "Accuracy of  PRAD: 100%\n",
      "Accuracy of  READ:  0%\n",
      "Accuracy of  THCA: 100%\n",
      "Accuracy of  UCEC: 84%\n"
     ]
    }
   ],
   "source": [
    "checkAccuracy(net, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     8] loss: 0.3739\n",
      "[1,    16] loss: 0.3289\n",
      "[1,    24] loss: 0.5001\n",
      "[1,    32] loss: 0.5046\n",
      "[1,    40] loss: 0.4178\n",
      "[1,    48] loss: 0.4871\n",
      "[1,    56] loss: 0.6646\n",
      "[1,    64] loss: 0.5542\n",
      "[1,    72] loss: 0.3801\n",
      "[1,    80] loss: 0.3853\n",
      "[1,    88] loss: 0.2009\n",
      "[1,    96] loss: 0.5450\n",
      "[1,   104] loss: 0.3825\n",
      "[1,   112] loss: 0.5381\n",
      "[1,   120] loss: 0.4720\n",
      "[1,   128] loss: 0.4223\n",
      "[1,   136] loss: 0.2920\n",
      "[1,   144] loss: 0.3814\n",
      "[1,   152] loss: 0.5882\n",
      "[1,   160] loss: 0.5881\n",
      "[1,   168] loss: 0.2842\n",
      "[1,   176] loss: 0.4630\n",
      "[1,   184] loss: 0.4509\n",
      "[1,   192] loss: 0.3327\n",
      "[1,   200] loss: 0.3658\n",
      "[1,   208] loss: 0.5523\n",
      "[1,   216] loss: 0.4592\n",
      "[1,   224] loss: 0.4303\n",
      "[1,   232] loss: 0.3617\n",
      "[1,   240] loss: 0.4880\n",
      "[1,   248] loss: 0.3932\n",
      "[1,   256] loss: 0.2802\n",
      "[1,   264] loss: 0.4423\n",
      "[1,   272] loss: 0.3278\n",
      "[1,   280] loss: 0.3437\n",
      "[1,   288] loss: 0.3680\n",
      "[1,   296] loss: 0.5083\n",
      "[1,   304] loss: 0.3781\n",
      "[1,   312] loss: 0.2145\n",
      "[1,   320] loss: 0.5688\n",
      "[1,   328] loss: 0.4237\n",
      "[1,   336] loss: 0.2710\n",
      "[1,   344] loss: 0.3926\n",
      "[1,   352] loss: 0.3507\n",
      "[1,   360] loss: 0.6946\n",
      "[1,   368] loss: 0.5929\n",
      "[1,   376] loss: 0.4740\n",
      "[1,   384] loss: 0.4066\n",
      "[1,   392] loss: 0.4950\n",
      "[1,   400] loss: 0.4374\n",
      "[1,   408] loss: 0.3007\n",
      "[1,   416] loss: 0.5132\n",
      "[1,   424] loss: 0.3433\n",
      "[1,   432] loss: 0.3479\n",
      "[1,   440] loss: 0.4034\n",
      "[1,   448] loss: 0.4491\n",
      "[1,   456] loss: 0.3172\n",
      "[1,   464] loss: 0.3983\n",
      "[1,   472] loss: 0.2905\n",
      "[2,     8] loss: 0.3988\n",
      "[2,    16] loss: 0.3499\n",
      "[2,    24] loss: 0.4819\n",
      "[2,    32] loss: 0.3703\n",
      "[2,    40] loss: 0.2376\n",
      "[2,    48] loss: 0.3593\n",
      "[2,    56] loss: 0.2254\n",
      "[2,    64] loss: 0.4902\n",
      "[2,    72] loss: 0.2273\n",
      "[2,    80] loss: 0.3091\n",
      "[2,    88] loss: 0.3048\n",
      "[2,    96] loss: 0.3978\n",
      "[2,   104] loss: 0.6109\n",
      "[2,   112] loss: 0.4597\n",
      "[2,   120] loss: 0.2051\n",
      "[2,   128] loss: 0.4514\n",
      "[2,   136] loss: 0.4341\n",
      "[2,   144] loss: 0.5803\n",
      "[2,   152] loss: 0.2860\n",
      "[2,   160] loss: 0.1913\n",
      "[2,   168] loss: 0.6223\n",
      "[2,   176] loss: 0.3096\n",
      "[2,   184] loss: 0.3825\n",
      "[2,   192] loss: 0.2823\n",
      "[2,   200] loss: 0.3731\n",
      "[2,   208] loss: 0.2853\n",
      "[2,   216] loss: 0.3555\n",
      "[2,   224] loss: 0.5370\n",
      "[2,   232] loss: 0.2618\n",
      "[2,   240] loss: 0.3430\n",
      "[2,   248] loss: 0.2794\n",
      "[2,   256] loss: 0.3580\n",
      "[2,   264] loss: 0.5471\n",
      "[2,   272] loss: 0.3014\n",
      "[2,   280] loss: 0.4068\n",
      "[2,   288] loss: 0.3522\n",
      "[2,   296] loss: 0.3424\n",
      "[2,   304] loss: 0.2737\n",
      "[2,   312] loss: 0.3388\n",
      "[2,   320] loss: 0.3816\n",
      "[2,   328] loss: 0.2660\n",
      "[2,   336] loss: 0.4349\n",
      "[2,   344] loss: 0.4286\n",
      "[2,   352] loss: 0.3832\n",
      "[2,   360] loss: 0.1991\n",
      "[2,   368] loss: 0.2723\n",
      "[2,   376] loss: 0.3982\n",
      "[2,   384] loss: 0.2196\n",
      "[2,   392] loss: 0.3596\n",
      "[2,   400] loss: 0.4815\n",
      "[2,   408] loss: 0.3549\n",
      "[2,   416] loss: 0.2509\n",
      "[2,   424] loss: 0.4508\n",
      "[2,   432] loss: 0.2545\n",
      "[2,   440] loss: 0.2617\n",
      "[2,   448] loss: 0.3958\n",
      "[2,   456] loss: 0.3638\n",
      "[2,   464] loss: 0.3750\n",
      "[2,   472] loss: 0.2970\n",
      "[3,     8] loss: 0.2726\n",
      "[3,    16] loss: 0.2735\n",
      "[3,    24] loss: 0.2486\n",
      "[3,    32] loss: 0.3886\n",
      "[3,    40] loss: 0.3017\n",
      "[3,    48] loss: 0.3521\n",
      "[3,    56] loss: 0.1606\n",
      "[3,    64] loss: 0.2014\n",
      "[3,    72] loss: 0.1672\n",
      "[3,    80] loss: 0.2594\n",
      "[3,    88] loss: 0.1746\n",
      "[3,    96] loss: 0.3791\n",
      "[3,   104] loss: 0.1819\n",
      "[3,   112] loss: 0.2066\n",
      "[3,   120] loss: 0.4401\n",
      "[3,   128] loss: 0.3680\n",
      "[3,   136] loss: 0.3134\n",
      "[3,   144] loss: 0.2496\n",
      "[3,   152] loss: 0.3295\n",
      "[3,   160] loss: 0.3267\n",
      "[3,   168] loss: 0.2986\n",
      "[3,   176] loss: 0.3950\n",
      "[3,   184] loss: 0.4246\n",
      "[3,   192] loss: 0.3166\n",
      "[3,   200] loss: 0.4685\n",
      "[3,   208] loss: 0.4230\n",
      "[3,   216] loss: 0.2223\n",
      "[3,   224] loss: 0.5085\n",
      "[3,   232] loss: 0.1334\n",
      "[3,   240] loss: 0.1631\n",
      "[3,   248] loss: 0.3425\n",
      "[3,   256] loss: 0.2477\n",
      "[3,   264] loss: 0.3236\n",
      "[3,   272] loss: 0.3100\n",
      "[3,   280] loss: 0.2350\n",
      "[3,   288] loss: 0.2407\n",
      "[3,   296] loss: 0.3463\n",
      "[3,   304] loss: 0.3143\n",
      "[3,   312] loss: 0.2352\n",
      "[3,   320] loss: 0.4651\n",
      "[3,   328] loss: 0.4074\n",
      "[3,   336] loss: 0.2432\n",
      "[3,   344] loss: 0.3643\n",
      "[3,   352] loss: 0.2463\n",
      "[3,   360] loss: 0.3965\n",
      "[3,   368] loss: 0.4223\n",
      "[3,   376] loss: 0.1983\n",
      "[3,   384] loss: 0.4675\n",
      "[3,   392] loss: 0.2651\n",
      "[3,   400] loss: 0.3494\n",
      "[3,   408] loss: 0.3825\n",
      "[3,   416] loss: 0.3943\n",
      "[3,   424] loss: 0.2432\n",
      "[3,   432] loss: 0.4183\n",
      "[3,   440] loss: 0.1801\n",
      "[3,   448] loss: 0.2342\n",
      "[3,   456] loss: 0.2427\n",
      "[3,   464] loss: 0.3456\n",
      "[3,   472] loss: 0.2172\n",
      "[4,     8] loss: 0.3544\n",
      "[4,    16] loss: 0.5465\n",
      "[4,    24] loss: 0.3630\n",
      "[4,    32] loss: 0.2997\n",
      "[4,    40] loss: 0.1965\n",
      "[4,    48] loss: 0.3447\n",
      "[4,    56] loss: 0.3159\n",
      "[4,    64] loss: 0.2620\n",
      "[4,    72] loss: 0.3058\n",
      "[4,    80] loss: 0.3768\n",
      "[4,    88] loss: 0.3978\n",
      "[4,    96] loss: 0.4999\n",
      "[4,   104] loss: 0.1711\n",
      "[4,   112] loss: 0.2964\n",
      "[4,   120] loss: 0.2509\n",
      "[4,   128] loss: 0.5355\n",
      "[4,   136] loss: 0.4279\n",
      "[4,   144] loss: 0.2706\n",
      "[4,   152] loss: 0.2260\n",
      "[4,   160] loss: 0.3866\n",
      "[4,   168] loss: 0.2711\n",
      "[4,   176] loss: 0.2814\n",
      "[4,   184] loss: 0.2068\n",
      "[4,   192] loss: 0.3718\n",
      "[4,   200] loss: 0.2370\n",
      "[4,   208] loss: 0.3994\n",
      "[4,   216] loss: 0.2047\n",
      "[4,   224] loss: 0.2180\n",
      "[4,   232] loss: 0.1592\n",
      "[4,   240] loss: 0.1649\n",
      "[4,   248] loss: 0.2145\n",
      "[4,   256] loss: 0.3424\n",
      "[4,   264] loss: 0.3138\n",
      "[4,   272] loss: 0.2749\n",
      "[4,   280] loss: 0.3259\n",
      "[4,   288] loss: 0.2555\n",
      "[4,   296] loss: 0.2232\n",
      "[4,   304] loss: 0.2467\n",
      "[4,   312] loss: 0.1889\n",
      "[4,   320] loss: 0.2852\n",
      "[4,   328] loss: 0.3574\n",
      "[4,   336] loss: 0.3028\n",
      "[4,   344] loss: 0.3621\n",
      "[4,   352] loss: 0.2278\n",
      "[4,   360] loss: 0.3200\n",
      "[4,   368] loss: 0.2549\n",
      "[4,   376] loss: 0.3656\n",
      "[4,   384] loss: 0.3018\n",
      "[4,   392] loss: 0.2087\n",
      "[4,   400] loss: 0.2843\n",
      "[4,   408] loss: 0.3401\n",
      "[4,   416] loss: 0.2759\n",
      "[4,   424] loss: 0.1945\n",
      "[4,   432] loss: 0.3503\n",
      "[4,   440] loss: 0.1735\n",
      "[4,   448] loss: 0.3284\n",
      "[4,   456] loss: 0.1948\n",
      "[4,   464] loss: 0.3087\n",
      "[4,   472] loss: 0.4738\n",
      "[5,     8] loss: 0.3315\n",
      "[5,    16] loss: 0.2197\n",
      "[5,    24] loss: 0.2559\n",
      "[5,    32] loss: 0.2891\n",
      "[5,    40] loss: 0.1918\n",
      "[5,    48] loss: 0.2468\n",
      "[5,    56] loss: 0.2863\n",
      "[5,    64] loss: 0.3511\n",
      "[5,    72] loss: 0.1993\n",
      "[5,    80] loss: 0.2797\n",
      "[5,    88] loss: 0.2049\n",
      "[5,    96] loss: 0.3117\n",
      "[5,   104] loss: 0.2639\n",
      "[5,   112] loss: 0.2998\n",
      "[5,   120] loss: 0.1607\n",
      "[5,   128] loss: 0.2301\n",
      "[5,   136] loss: 0.1893\n",
      "[5,   144] loss: 0.2555\n",
      "[5,   152] loss: 0.3361\n",
      "[5,   160] loss: 0.3378\n",
      "[5,   168] loss: 0.1602\n",
      "[5,   176] loss: 0.2845\n",
      "[5,   184] loss: 0.5936\n",
      "[5,   192] loss: 0.1866\n",
      "[5,   200] loss: 0.2405\n",
      "[5,   208] loss: 0.3041\n",
      "[5,   216] loss: 0.2063\n",
      "[5,   224] loss: 0.2551\n",
      "[5,   232] loss: 0.2144\n",
      "[5,   240] loss: 0.3890\n",
      "[5,   248] loss: 0.3060\n",
      "[5,   256] loss: 0.4690\n",
      "[5,   264] loss: 0.2631\n",
      "[5,   272] loss: 0.2662\n",
      "[5,   280] loss: 0.3409\n",
      "[5,   288] loss: 0.1609\n",
      "[5,   296] loss: 0.3004\n",
      "[5,   304] loss: 0.3457\n",
      "[5,   312] loss: 0.3245\n",
      "[5,   320] loss: 0.1986\n",
      "[5,   328] loss: 0.2087\n",
      "[5,   336] loss: 0.1960\n",
      "[5,   344] loss: 0.3711\n",
      "[5,   352] loss: 0.3879\n",
      "[5,   360] loss: 0.2300\n",
      "[5,   368] loss: 0.4234\n",
      "[5,   376] loss: 0.2908\n",
      "[5,   384] loss: 0.2111\n",
      "[5,   392] loss: 0.2090\n",
      "[5,   400] loss: 0.2648\n",
      "[5,   408] loss: 0.2189\n",
      "[5,   416] loss: 0.4500\n",
      "[5,   424] loss: 0.2750\n",
      "[5,   432] loss: 0.2484\n",
      "[5,   440] loss: 0.2545\n",
      "[5,   448] loss: 0.3002\n",
      "[5,   456] loss: 0.3387\n",
      "[5,   464] loss: 0.1793\n",
      "[5,   472] loss: 0.2643\n",
      "[6,     8] loss: 0.0986\n",
      "[6,    16] loss: 0.1832\n",
      "[6,    24] loss: 0.3340\n",
      "[6,    32] loss: 0.5556\n",
      "[6,    40] loss: 0.2340\n",
      "[6,    48] loss: 0.2392\n",
      "[6,    56] loss: 0.3403\n",
      "[6,    64] loss: 0.2772\n",
      "[6,    72] loss: 0.2076\n",
      "[6,    80] loss: 0.3331\n",
      "[6,    88] loss: 0.1988\n",
      "[6,    96] loss: 0.2700\n",
      "[6,   104] loss: 0.3390\n",
      "[6,   112] loss: 0.2726\n",
      "[6,   120] loss: 0.3880\n",
      "[6,   128] loss: 0.1333\n",
      "[6,   136] loss: 0.2134\n",
      "[6,   144] loss: 0.1360\n",
      "[6,   152] loss: 0.2081\n",
      "[6,   160] loss: 0.3501\n",
      "[6,   168] loss: 0.2530\n",
      "[6,   176] loss: 0.3029\n",
      "[6,   184] loss: 0.3182\n",
      "[6,   192] loss: 0.1783\n",
      "[6,   200] loss: 0.3136\n",
      "[6,   208] loss: 0.1482\n",
      "[6,   216] loss: 0.2626\n",
      "[6,   224] loss: 0.1903\n",
      "[6,   232] loss: 0.2639\n",
      "[6,   240] loss: 0.3481\n",
      "[6,   248] loss: 0.2569\n",
      "[6,   256] loss: 0.3140\n",
      "[6,   264] loss: 0.0922\n",
      "[6,   272] loss: 0.2983\n",
      "[6,   280] loss: 0.3117\n",
      "[6,   288] loss: 0.3356\n",
      "[6,   296] loss: 0.1995\n",
      "[6,   304] loss: 0.2506\n",
      "[6,   312] loss: 0.4343\n",
      "[6,   320] loss: 0.2536\n",
      "[6,   328] loss: 0.3080\n",
      "[6,   336] loss: 0.2484\n",
      "[6,   344] loss: 0.3984\n",
      "[6,   352] loss: 0.1717\n",
      "[6,   360] loss: 0.3072\n",
      "[6,   368] loss: 0.2779\n",
      "[6,   376] loss: 0.2496\n",
      "[6,   384] loss: 0.2672\n",
      "[6,   392] loss: 0.3190\n",
      "[6,   400] loss: 0.1988\n",
      "[6,   408] loss: 0.3382\n",
      "[6,   416] loss: 0.2054\n",
      "[6,   424] loss: 0.1297\n",
      "[6,   432] loss: 0.1570\n",
      "[6,   440] loss: 0.1791\n",
      "[6,   448] loss: 0.2620\n",
      "[6,   456] loss: 0.2107\n",
      "[6,   464] loss: 0.2177\n",
      "[6,   472] loss: 0.2397\n",
      "[7,     8] loss: 0.3116\n",
      "[7,    16] loss: 0.1335\n",
      "[7,    24] loss: 0.3291\n",
      "[7,    32] loss: 0.2385\n",
      "[7,    40] loss: 0.3651\n",
      "[7,    48] loss: 0.3245\n",
      "[7,    56] loss: 0.2782\n",
      "[7,    64] loss: 0.2411\n",
      "[7,    72] loss: 0.0872\n",
      "[7,    80] loss: 0.1757\n",
      "[7,    88] loss: 0.2884\n",
      "[7,    96] loss: 0.2814\n",
      "[7,   104] loss: 0.3168\n",
      "[7,   112] loss: 0.2295\n",
      "[7,   120] loss: 0.2256\n",
      "[7,   128] loss: 0.1100\n",
      "[7,   136] loss: 0.1841\n",
      "[7,   144] loss: 0.2459\n",
      "[7,   152] loss: 0.1486\n",
      "[7,   160] loss: 0.2569\n",
      "[7,   168] loss: 0.2466\n",
      "[7,   176] loss: 0.2487\n",
      "[7,   184] loss: 0.0735\n",
      "[7,   192] loss: 0.2219\n",
      "[7,   200] loss: 0.1967\n",
      "[7,   208] loss: 0.0905\n",
      "[7,   216] loss: 0.3877\n",
      "[7,   224] loss: 0.2011\n",
      "[7,   232] loss: 0.1878\n",
      "[7,   240] loss: 0.2565\n",
      "[7,   248] loss: 0.1998\n",
      "[7,   256] loss: 0.2713\n",
      "[7,   264] loss: 0.2973\n",
      "[7,   272] loss: 0.2502\n",
      "[7,   280] loss: 0.1381\n",
      "[7,   288] loss: 0.2759\n",
      "[7,   296] loss: 0.2800\n",
      "[7,   304] loss: 0.2548\n",
      "[7,   312] loss: 0.2754\n",
      "[7,   320] loss: 0.2478\n",
      "[7,   328] loss: 0.3341\n",
      "[7,   336] loss: 0.1968\n",
      "[7,   344] loss: 0.2010\n",
      "[7,   352] loss: 0.1537\n",
      "[7,   360] loss: 0.1493\n",
      "[7,   368] loss: 0.5254\n",
      "[7,   376] loss: 0.2305\n",
      "[7,   384] loss: 0.1873\n",
      "[7,   392] loss: 0.4954\n",
      "[7,   400] loss: 0.2722\n",
      "[7,   408] loss: 0.2639\n",
      "[7,   416] loss: 0.1753\n",
      "[7,   424] loss: 0.1347\n",
      "[7,   432] loss: 0.2425\n",
      "[7,   440] loss: 0.1599\n",
      "[7,   448] loss: 0.3366\n",
      "[7,   456] loss: 0.2874\n",
      "[7,   464] loss: 0.2109\n",
      "[7,   472] loss: 0.2254\n",
      "Finished\n",
      "CPU times: user 1min 12s, sys: 28.9 s, total: 1min 41s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%time trainRun(net, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: \n",
      "Accuracy of  BRCA: 100%\n",
      "Accuracy of  COAD: 100%\n",
      "Accuracy of   GBM: 100%\n",
      "Accuracy of  HNSC: 96%\n",
      "Accuracy of  KIRC: 98%\n",
      "Accuracy of  KIRP: 96%\n",
      "Accuracy of   LGG: 100%\n",
      "Accuracy of  LUAD: 99%\n",
      "Accuracy of  LUSC: 89%\n",
      "Accuracy of  PRAD: 100%\n",
      "Accuracy of  READ:  0%\n",
      "Accuracy of  THCA: 100%\n",
      "Accuracy of  UCEC: 100%\n",
      "Validation set accuracy: \n",
      "Accuracy of  BRCA: 98%\n",
      "Accuracy of  COAD: 100%\n",
      "Accuracy of   GBM: 100%\n",
      "Accuracy of  HNSC: 83%\n",
      "Accuracy of  KIRC: 92%\n",
      "Accuracy of  KIRP: 90%\n",
      "Accuracy of   LGG: 93%\n",
      "Accuracy of  LUAD: 89%\n",
      "Accuracy of  LUSC: 71%\n",
      "Accuracy of  PRAD: 100%\n",
      "Accuracy of  READ:  0%\n",
      "Accuracy of  THCA: 97%\n",
      "Accuracy of  UCEC: 97%\n"
     ]
    }
   ],
   "source": [
    "checkAccuracy(net, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     8] loss: 0.1032\n",
      "[1,    16] loss: 0.3389\n",
      "[1,    24] loss: 0.1226\n",
      "[1,    32] loss: 0.2883\n",
      "[1,    40] loss: 0.0695\n",
      "[1,    48] loss: 0.1461\n",
      "[1,    56] loss: 0.1214\n",
      "[1,    64] loss: 0.1479\n",
      "[1,    72] loss: 0.1837\n",
      "[1,    80] loss: 0.1787\n",
      "[1,    88] loss: 0.1068\n",
      "[1,    96] loss: 0.2188\n",
      "[1,   104] loss: 0.1839\n",
      "[1,   112] loss: 0.1377\n",
      "[1,   120] loss: 0.1528\n",
      "[1,   128] loss: 0.2050\n",
      "[1,   136] loss: 0.3564\n",
      "[1,   144] loss: 0.1659\n",
      "[1,   152] loss: 0.2705\n",
      "[1,   160] loss: 0.2162\n",
      "[1,   168] loss: 0.2342\n",
      "[1,   176] loss: 0.2039\n",
      "[1,   184] loss: 0.4902\n",
      "[1,   192] loss: 0.2576\n",
      "[1,   200] loss: 0.2223\n",
      "[1,   208] loss: 0.2664\n",
      "[1,   216] loss: 0.2057\n",
      "[1,   224] loss: 0.3157\n",
      "[1,   232] loss: 0.2379\n",
      "[1,   240] loss: 0.2991\n",
      "[1,   248] loss: 0.1634\n",
      "[1,   256] loss: 0.1911\n",
      "[1,   264] loss: 0.2762\n",
      "[1,   272] loss: 0.1543\n",
      "[1,   280] loss: 0.1434\n",
      "[1,   288] loss: 0.2199\n",
      "[1,   296] loss: 0.1229\n",
      "[1,   304] loss: 0.1883\n",
      "[1,   312] loss: 0.1857\n",
      "[1,   320] loss: 0.1555\n",
      "[1,   328] loss: 0.2272\n",
      "[1,   336] loss: 0.3053\n",
      "[1,   344] loss: 0.2387\n",
      "[1,   352] loss: 0.3559\n",
      "[1,   360] loss: 0.2669\n",
      "[1,   368] loss: 0.3245\n",
      "[1,   376] loss: 0.3017\n",
      "[1,   384] loss: 0.2101\n",
      "[1,   392] loss: 0.4151\n",
      "[1,   400] loss: 0.2303\n",
      "[1,   408] loss: 0.1720\n",
      "[1,   416] loss: 0.1880\n",
      "[1,   424] loss: 0.3192\n",
      "[1,   432] loss: 0.3360\n",
      "[1,   440] loss: 0.2852\n",
      "[1,   448] loss: 0.1075\n",
      "[1,   456] loss: 0.1004\n",
      "[1,   464] loss: 0.2797\n",
      "[1,   472] loss: 0.0643\n",
      "[2,     8] loss: 0.2042\n",
      "[2,    16] loss: 0.2511\n",
      "[2,    24] loss: 0.2451\n",
      "[2,    32] loss: 0.1816\n",
      "[2,    40] loss: 0.2274\n",
      "[2,    48] loss: 0.0978\n",
      "[2,    56] loss: 0.2739\n",
      "[2,    64] loss: 0.1623\n",
      "[2,    72] loss: 0.2792\n",
      "[2,    80] loss: 0.1887\n",
      "[2,    88] loss: 0.1614\n",
      "[2,    96] loss: 0.1977\n",
      "[2,   104] loss: 0.1322\n",
      "[2,   112] loss: 0.1246\n",
      "[2,   120] loss: 0.1511\n",
      "[2,   128] loss: 0.3875\n",
      "[2,   136] loss: 0.3432\n",
      "[2,   144] loss: 0.2957\n",
      "[2,   152] loss: 0.3733\n",
      "[2,   160] loss: 0.3553\n",
      "[2,   168] loss: 0.3594\n",
      "[2,   176] loss: 0.1926\n",
      "[2,   184] loss: 0.1490\n",
      "[2,   192] loss: 0.1678\n",
      "[2,   200] loss: 0.2451\n",
      "[2,   208] loss: 0.2398\n",
      "[2,   216] loss: 0.1960\n",
      "[2,   224] loss: 0.3683\n",
      "[2,   232] loss: 0.2409\n",
      "[2,   240] loss: 0.2444\n",
      "[2,   248] loss: 0.2657\n",
      "[2,   256] loss: 0.2280\n",
      "[2,   264] loss: 0.1930\n",
      "[2,   272] loss: 0.1802\n",
      "[2,   280] loss: 0.1841\n",
      "[2,   288] loss: 0.1918\n",
      "[2,   296] loss: 0.2515\n",
      "[2,   304] loss: 0.1138\n",
      "[2,   312] loss: 0.1119\n",
      "[2,   320] loss: 0.1675\n",
      "[2,   328] loss: 0.1496\n",
      "[2,   336] loss: 0.1908\n",
      "[2,   344] loss: 0.0978\n",
      "[2,   352] loss: 0.3180\n",
      "[2,   360] loss: 0.1895\n",
      "[2,   368] loss: 0.1812\n",
      "[2,   376] loss: 0.4075\n",
      "[2,   384] loss: 0.2901\n",
      "[2,   392] loss: 0.2972\n",
      "[2,   400] loss: 0.1911\n",
      "[2,   408] loss: 0.1135\n",
      "[2,   416] loss: 0.2213\n",
      "[2,   424] loss: 0.2295\n",
      "[2,   432] loss: 0.2471\n",
      "[2,   440] loss: 0.0469\n",
      "[2,   448] loss: 0.3008\n",
      "[2,   456] loss: 0.3073\n",
      "[2,   464] loss: 0.1997\n",
      "[2,   472] loss: 0.1136\n",
      "[3,     8] loss: 0.0662\n",
      "[3,    16] loss: 0.1788\n",
      "[3,    24] loss: 0.2616\n",
      "[3,    32] loss: 0.2842\n",
      "[3,    40] loss: 0.3825\n",
      "[3,    48] loss: 0.2122\n",
      "[3,    56] loss: 0.1617\n",
      "[3,    64] loss: 0.1578\n",
      "[3,    72] loss: 0.1084\n",
      "[3,    80] loss: 0.1583\n",
      "[3,    88] loss: 0.1986\n",
      "[3,    96] loss: 0.2443\n",
      "[3,   104] loss: 0.4574\n",
      "[3,   112] loss: 0.2094\n",
      "[3,   120] loss: 0.1867\n",
      "[3,   128] loss: 0.1822\n",
      "[3,   136] loss: 0.1617\n",
      "[3,   144] loss: 0.3368\n",
      "[3,   152] loss: 0.2372\n",
      "[3,   160] loss: 0.1949\n",
      "[3,   168] loss: 0.3577\n",
      "[3,   176] loss: 0.1224\n",
      "[3,   184] loss: 0.2217\n",
      "[3,   192] loss: 0.2060\n",
      "[3,   200] loss: 0.0966\n",
      "[3,   208] loss: 0.3383\n",
      "[3,   216] loss: 0.2008\n",
      "[3,   224] loss: 0.2065\n",
      "[3,   232] loss: 0.2212\n",
      "[3,   240] loss: 0.2697\n",
      "[3,   248] loss: 0.0845\n",
      "[3,   256] loss: 0.2306\n",
      "[3,   264] loss: 0.1828\n",
      "[3,   272] loss: 0.1477\n",
      "[3,   280] loss: 0.3053\n",
      "[3,   288] loss: 0.1411\n",
      "[3,   296] loss: 0.2330\n",
      "[3,   304] loss: 0.3767\n",
      "[3,   312] loss: 0.1950\n",
      "[3,   320] loss: 0.3444\n",
      "[3,   328] loss: 0.1948\n",
      "[3,   336] loss: 0.1828\n",
      "[3,   344] loss: 0.4356\n",
      "[3,   352] loss: 0.3599\n",
      "[3,   360] loss: 0.1305\n",
      "[3,   368] loss: 0.1332\n",
      "[3,   376] loss: 0.2179\n",
      "[3,   384] loss: 0.1672\n",
      "[3,   392] loss: 0.2329\n",
      "[3,   400] loss: 0.0812\n",
      "[3,   408] loss: 0.2123\n",
      "[3,   416] loss: 0.1486\n",
      "[3,   424] loss: 0.0655\n",
      "[3,   432] loss: 0.1975\n",
      "[3,   440] loss: 0.2803\n",
      "[3,   448] loss: 0.2389\n",
      "[3,   456] loss: 0.2287\n",
      "[3,   464] loss: 0.1421\n",
      "[3,   472] loss: 0.1688\n",
      "[4,     8] loss: 0.1181\n",
      "[4,    16] loss: 0.1452\n",
      "[4,    24] loss: 0.3430\n",
      "[4,    32] loss: 0.2036\n",
      "[4,    40] loss: 0.2241\n",
      "[4,    48] loss: 0.3353\n",
      "[4,    56] loss: 0.0885\n",
      "[4,    64] loss: 0.1009\n",
      "[4,    72] loss: 0.2757\n",
      "[4,    80] loss: 0.2132\n",
      "[4,    88] loss: 0.1416\n",
      "[4,    96] loss: 0.1536\n",
      "[4,   104] loss: 0.3792\n",
      "[4,   112] loss: 0.1356\n",
      "[4,   120] loss: 0.1133\n",
      "[4,   128] loss: 0.2166\n",
      "[4,   136] loss: 0.1281\n",
      "[4,   144] loss: 0.1274\n",
      "[4,   152] loss: 0.1004\n",
      "[4,   160] loss: 0.2295\n",
      "[4,   168] loss: 0.1864\n",
      "[4,   176] loss: 0.1625\n",
      "[4,   184] loss: 0.1878\n",
      "[4,   192] loss: 0.2433\n",
      "[4,   200] loss: 0.2210\n",
      "[4,   208] loss: 0.2243\n",
      "[4,   216] loss: 0.1463\n",
      "[4,   224] loss: 0.1602\n",
      "[4,   232] loss: 0.1623\n",
      "[4,   240] loss: 0.1571\n",
      "[4,   248] loss: 0.1092\n",
      "[4,   256] loss: 0.1592\n",
      "[4,   264] loss: 0.1445\n",
      "[4,   272] loss: 0.2248\n",
      "[4,   280] loss: 0.1629\n",
      "[4,   288] loss: 0.1677\n",
      "[4,   296] loss: 0.1990\n",
      "[4,   304] loss: 0.1667\n",
      "[4,   312] loss: 0.2491\n",
      "[4,   320] loss: 0.0976\n",
      "[4,   328] loss: 0.1774\n",
      "[4,   336] loss: 0.0956\n",
      "[4,   344] loss: 0.1662\n",
      "[4,   352] loss: 0.0891\n",
      "[4,   360] loss: 0.1745\n",
      "[4,   368] loss: 0.0946\n",
      "[4,   376] loss: 0.1252\n",
      "[4,   384] loss: 0.1312\n",
      "[4,   392] loss: 0.2725\n",
      "[4,   400] loss: 0.3061\n",
      "[4,   408] loss: 0.2853\n",
      "[4,   416] loss: 0.1396\n",
      "[4,   424] loss: 0.1845\n",
      "[4,   432] loss: 0.1373\n",
      "[4,   440] loss: 0.1784\n",
      "[4,   448] loss: 0.2661\n",
      "[4,   456] loss: 0.2815\n",
      "[4,   464] loss: 0.1370\n",
      "[4,   472] loss: 0.2523\n",
      "[5,     8] loss: 0.1504\n",
      "[5,    16] loss: 0.2461\n",
      "[5,    24] loss: 0.1535\n",
      "[5,    32] loss: 0.1292\n",
      "[5,    40] loss: 0.2532\n",
      "[5,    48] loss: 0.2506\n",
      "[5,    56] loss: 0.1806\n",
      "[5,    64] loss: 0.3024\n",
      "[5,    72] loss: 0.2700\n",
      "[5,    80] loss: 0.2030\n",
      "[5,    88] loss: 0.2008\n",
      "[5,    96] loss: 0.2474\n",
      "[5,   104] loss: 0.1525\n",
      "[5,   112] loss: 0.2243\n",
      "[5,   120] loss: 0.0935\n",
      "[5,   128] loss: 0.1365\n",
      "[5,   136] loss: 0.0807\n",
      "[5,   144] loss: 0.3714\n",
      "[5,   152] loss: 0.2846\n",
      "[5,   160] loss: 0.3429\n",
      "[5,   168] loss: 0.2708\n",
      "[5,   176] loss: 0.1789\n",
      "[5,   184] loss: 0.1850\n",
      "[5,   192] loss: 0.0574\n",
      "[5,   200] loss: 0.1310\n",
      "[5,   208] loss: 0.1859\n",
      "[5,   216] loss: 0.1838\n",
      "[5,   224] loss: 0.1980\n",
      "[5,   232] loss: 0.4875\n",
      "[5,   240] loss: 0.2541\n",
      "[5,   248] loss: 0.2369\n",
      "[5,   256] loss: 0.1099\n",
      "[5,   264] loss: 0.3045\n",
      "[5,   272] loss: 0.1737\n",
      "[5,   280] loss: 0.1799\n",
      "[5,   288] loss: 0.2249\n",
      "[5,   296] loss: 0.1841\n",
      "[5,   304] loss: 0.1637\n",
      "[5,   312] loss: 0.1758\n",
      "[5,   320] loss: 0.2120\n",
      "[5,   328] loss: 0.1644\n",
      "[5,   336] loss: 0.2073\n",
      "[5,   344] loss: 0.1970\n",
      "[5,   352] loss: 0.4552\n",
      "[5,   360] loss: 0.1208\n",
      "[5,   368] loss: 0.1086\n",
      "[5,   376] loss: 0.2370\n",
      "[5,   384] loss: 0.1770\n",
      "[5,   392] loss: 0.1919\n",
      "[5,   400] loss: 0.0953\n",
      "[5,   408] loss: 0.0360\n",
      "[5,   416] loss: 0.4128\n",
      "[5,   424] loss: 0.1368\n",
      "[5,   432] loss: 0.2125\n",
      "[5,   440] loss: 0.2164\n",
      "[5,   448] loss: 0.1265\n",
      "[5,   456] loss: 0.2472\n",
      "[5,   464] loss: 0.3367\n",
      "[5,   472] loss: 0.2878\n",
      "[6,     8] loss: 0.2637\n",
      "[6,    16] loss: 0.1379\n",
      "[6,    24] loss: 0.1581\n",
      "[6,    32] loss: 0.2191\n",
      "[6,    40] loss: 0.3258\n",
      "[6,    48] loss: 0.0997\n",
      "[6,    56] loss: 0.1329\n",
      "[6,    64] loss: 0.2583\n",
      "[6,    72] loss: 0.1759\n",
      "[6,    80] loss: 0.2001\n",
      "[6,    88] loss: 0.1948\n",
      "[6,    96] loss: 0.2287\n",
      "[6,   104] loss: 0.2184\n",
      "[6,   112] loss: 0.1940\n",
      "[6,   120] loss: 0.1460\n",
      "[6,   128] loss: 0.3205\n",
      "[6,   136] loss: 0.2226\n",
      "[6,   144] loss: 0.1251\n",
      "[6,   152] loss: 0.2897\n",
      "[6,   160] loss: 0.2350\n",
      "[6,   168] loss: 0.2589\n",
      "[6,   176] loss: 0.2071\n",
      "[6,   184] loss: 0.1970\n",
      "[6,   192] loss: 0.1504\n",
      "[6,   200] loss: 0.2009\n",
      "[6,   208] loss: 0.2309\n",
      "[6,   216] loss: 0.1188\n",
      "[6,   224] loss: 0.0653\n",
      "[6,   232] loss: 0.3836\n",
      "[6,   240] loss: 0.1277\n",
      "[6,   248] loss: 0.1646\n",
      "[6,   256] loss: 0.1712\n",
      "[6,   264] loss: 0.3193\n",
      "[6,   272] loss: 0.1765\n",
      "[6,   280] loss: 0.2100\n",
      "[6,   288] loss: 0.2267\n",
      "[6,   296] loss: 0.1374\n",
      "[6,   304] loss: 0.2440\n",
      "[6,   312] loss: 0.0701\n",
      "[6,   320] loss: 0.2268\n",
      "[6,   328] loss: 0.1408\n",
      "[6,   336] loss: 0.2354\n",
      "[6,   344] loss: 0.2667\n",
      "[6,   352] loss: 0.2093\n",
      "[6,   360] loss: 0.4057\n",
      "[6,   368] loss: 0.2092\n",
      "[6,   376] loss: 0.1178\n",
      "[6,   384] loss: 0.1121\n",
      "[6,   392] loss: 0.1725\n",
      "[6,   400] loss: 0.1477\n",
      "[6,   408] loss: 0.1829\n",
      "[6,   416] loss: 0.1936\n",
      "[6,   424] loss: 0.1111\n",
      "[6,   432] loss: 0.1729\n",
      "[6,   440] loss: 0.1877\n",
      "[6,   448] loss: 0.1167\n",
      "[6,   456] loss: 0.1058\n",
      "[6,   464] loss: 0.2070\n",
      "[6,   472] loss: 0.1791\n",
      "[7,     8] loss: 0.1121\n",
      "[7,    16] loss: 0.1666\n",
      "[7,    24] loss: 0.2488\n",
      "[7,    32] loss: 0.1614\n",
      "[7,    40] loss: 0.0548\n",
      "[7,    48] loss: 0.2175\n",
      "[7,    56] loss: 0.1978\n",
      "[7,    64] loss: 0.1765\n",
      "[7,    72] loss: 0.1519\n",
      "[7,    80] loss: 0.2516\n",
      "[7,    88] loss: 0.1352\n",
      "[7,    96] loss: 0.1269\n",
      "[7,   104] loss: 0.0777\n",
      "[7,   112] loss: 0.2075\n",
      "[7,   120] loss: 0.1889\n",
      "[7,   128] loss: 0.1752\n",
      "[7,   136] loss: 0.2325\n",
      "[7,   144] loss: 0.1600\n",
      "[7,   152] loss: 0.0929\n",
      "[7,   160] loss: 0.1900\n",
      "[7,   168] loss: 0.1414\n",
      "[7,   176] loss: 0.1875\n",
      "[7,   184] loss: 0.2985\n",
      "[7,   192] loss: 0.1419\n",
      "[7,   200] loss: 0.1175\n",
      "[7,   208] loss: 0.2252\n",
      "[7,   216] loss: 0.2653\n",
      "[7,   224] loss: 0.1372\n",
      "[7,   232] loss: 0.1305\n",
      "[7,   240] loss: 0.0941\n",
      "[7,   248] loss: 0.2276\n",
      "[7,   256] loss: 0.1014\n",
      "[7,   264] loss: 0.1351\n",
      "[7,   272] loss: 0.2776\n",
      "[7,   280] loss: 0.2732\n",
      "[7,   288] loss: 0.1885\n",
      "[7,   296] loss: 0.1895\n",
      "[7,   304] loss: 0.1183\n",
      "[7,   312] loss: 0.1227\n",
      "[7,   320] loss: 0.1695\n",
      "[7,   328] loss: 0.1705\n",
      "[7,   336] loss: 0.1596\n",
      "[7,   344] loss: 0.1287\n",
      "[7,   352] loss: 0.0931\n",
      "[7,   360] loss: 0.1287\n",
      "[7,   368] loss: 0.1958\n",
      "[7,   376] loss: 0.2524\n",
      "[7,   384] loss: 0.2395\n",
      "[7,   392] loss: 0.1689\n",
      "[7,   400] loss: 0.2555\n",
      "[7,   408] loss: 0.1381\n",
      "[7,   416] loss: 0.0869\n",
      "[7,   424] loss: 0.1575\n",
      "[7,   432] loss: 0.2103\n",
      "[7,   440] loss: 0.1461\n",
      "[7,   448] loss: 0.1406\n",
      "[7,   456] loss: 0.1425\n",
      "[7,   464] loss: 0.1677\n",
      "[7,   472] loss: 0.1328\n",
      "[8,     8] loss: 0.1099\n",
      "[8,    16] loss: 0.1027\n",
      "[8,    24] loss: 0.1528\n",
      "[8,    32] loss: 0.0899\n",
      "[8,    40] loss: 0.1995\n",
      "[8,    48] loss: 0.2394\n",
      "[8,    56] loss: 0.2633\n",
      "[8,    64] loss: 0.3903\n",
      "[8,    72] loss: 0.1390\n",
      "[8,    80] loss: 0.1565\n",
      "[8,    88] loss: 0.3236\n",
      "[8,    96] loss: 0.1184\n",
      "[8,   104] loss: 0.1489\n",
      "[8,   112] loss: 0.2887\n",
      "[8,   120] loss: 0.1803\n",
      "[8,   128] loss: 0.1801\n",
      "[8,   136] loss: 0.0825\n",
      "[8,   144] loss: 0.2265\n",
      "[8,   152] loss: 0.1476\n",
      "[8,   160] loss: 0.0807\n",
      "[8,   168] loss: 0.2527\n",
      "[8,   176] loss: 0.1259\n",
      "[8,   184] loss: 0.2054\n",
      "[8,   192] loss: 0.1685\n",
      "[8,   200] loss: 0.1068\n",
      "[8,   208] loss: 0.0968\n",
      "[8,   216] loss: 0.3665\n",
      "[8,   224] loss: 0.1138\n",
      "[8,   232] loss: 0.1762\n",
      "[8,   240] loss: 0.1726\n",
      "[8,   248] loss: 0.1141\n",
      "[8,   256] loss: 0.2282\n",
      "[8,   264] loss: 0.2430\n",
      "[8,   272] loss: 0.2166\n",
      "[8,   280] loss: 0.1242\n",
      "[8,   288] loss: 0.0883\n",
      "[8,   296] loss: 0.1546\n",
      "[8,   304] loss: 0.1635\n",
      "[8,   312] loss: 0.1654\n",
      "[8,   320] loss: 0.0949\n",
      "[8,   328] loss: 0.1792\n",
      "[8,   336] loss: 0.3476\n",
      "[8,   344] loss: 0.3582\n",
      "[8,   352] loss: 0.2881\n",
      "[8,   360] loss: 0.2952\n",
      "[8,   368] loss: 0.3121\n",
      "[8,   376] loss: 0.1355\n",
      "[8,   384] loss: 0.1624\n",
      "[8,   392] loss: 0.2908\n",
      "[8,   400] loss: 0.1709\n",
      "[8,   408] loss: 0.1259\n",
      "[8,   416] loss: 0.1870\n",
      "[8,   424] loss: 0.2198\n",
      "[8,   432] loss: 0.1190\n",
      "[8,   440] loss: 0.1361\n",
      "[8,   448] loss: 0.2137\n",
      "[8,   456] loss: 0.1175\n",
      "[8,   464] loss: 0.2386\n",
      "[8,   472] loss: 0.1161\n",
      "[9,     8] loss: 0.1491\n",
      "[9,    16] loss: 0.3035\n",
      "[9,    24] loss: 0.1251\n",
      "[9,    32] loss: 0.0933\n",
      "[9,    40] loss: 0.1180\n",
      "[9,    48] loss: 0.1445\n",
      "[9,    56] loss: 0.1559\n",
      "[9,    64] loss: 0.1132\n",
      "[9,    72] loss: 0.0981\n",
      "[9,    80] loss: 0.4117\n",
      "[9,    88] loss: 0.2300\n",
      "[9,    96] loss: 0.3265\n",
      "[9,   104] loss: 0.1450\n",
      "[9,   112] loss: 0.1079\n",
      "[9,   120] loss: 0.1485\n",
      "[9,   128] loss: 0.2046\n",
      "[9,   136] loss: 0.1162\n",
      "[9,   144] loss: 0.1242\n",
      "[9,   152] loss: 0.1749\n",
      "[9,   160] loss: 0.1132\n",
      "[9,   168] loss: 0.1049\n",
      "[9,   176] loss: 0.0959\n",
      "[9,   184] loss: 0.2120\n",
      "[9,   192] loss: 0.1816\n",
      "[9,   200] loss: 0.2405\n",
      "[9,   208] loss: 0.3257\n",
      "[9,   216] loss: 0.1334\n",
      "[9,   224] loss: 0.1636\n",
      "[9,   232] loss: 0.1189\n",
      "[9,   240] loss: 0.2212\n",
      "[9,   248] loss: 0.0451\n",
      "[9,   256] loss: 0.2631\n",
      "[9,   264] loss: 0.0650\n",
      "[9,   272] loss: 0.1894\n",
      "[9,   280] loss: 0.0686\n",
      "[9,   288] loss: 0.1509\n",
      "[9,   296] loss: 0.0455\n",
      "[9,   304] loss: 0.0877\n",
      "[9,   312] loss: 0.1263\n",
      "[9,   320] loss: 0.0571\n",
      "[9,   328] loss: 0.3301\n",
      "[9,   336] loss: 0.2484\n",
      "[9,   344] loss: 0.1613\n",
      "[9,   352] loss: 0.1414\n",
      "[9,   360] loss: 0.0903\n",
      "[9,   368] loss: 0.3050\n",
      "[9,   376] loss: 0.1658\n",
      "[9,   384] loss: 0.0767\n",
      "[9,   392] loss: 0.1902\n",
      "[9,   400] loss: 0.1186\n",
      "[9,   408] loss: 0.2440\n",
      "[9,   416] loss: 0.1443\n",
      "[9,   424] loss: 0.1870\n",
      "[9,   432] loss: 0.1055\n",
      "[9,   440] loss: 0.0859\n",
      "[9,   448] loss: 0.0436\n",
      "[9,   456] loss: 0.3011\n",
      "[9,   464] loss: 0.2639\n",
      "[9,   472] loss: 0.2265\n",
      "[10,     8] loss: 0.1240\n",
      "[10,    16] loss: 0.3141\n",
      "[10,    24] loss: 0.1155\n",
      "[10,    32] loss: 0.1249\n",
      "[10,    40] loss: 0.2070\n",
      "[10,    48] loss: 0.1317\n",
      "[10,    56] loss: 0.1018\n",
      "[10,    64] loss: 0.1495\n",
      "[10,    72] loss: 0.1674\n",
      "[10,    80] loss: 0.0866\n",
      "[10,    88] loss: 0.1104\n",
      "[10,    96] loss: 0.1593\n",
      "[10,   104] loss: 0.1456\n",
      "[10,   112] loss: 0.1518\n",
      "[10,   120] loss: 0.1741\n",
      "[10,   128] loss: 0.0812\n",
      "[10,   136] loss: 0.0819\n",
      "[10,   144] loss: 0.2041\n",
      "[10,   152] loss: 0.2224\n",
      "[10,   160] loss: 0.0870\n",
      "[10,   168] loss: 0.0372\n",
      "[10,   176] loss: 0.1311\n",
      "[10,   184] loss: 0.0621\n",
      "[10,   192] loss: 0.1570\n",
      "[10,   200] loss: 0.1326\n",
      "[10,   208] loss: 0.1411\n",
      "[10,   216] loss: 0.1863\n",
      "[10,   224] loss: 0.2182\n",
      "[10,   232] loss: 0.2136\n",
      "[10,   240] loss: 0.1441\n",
      "[10,   248] loss: 0.1969\n",
      "[10,   256] loss: 0.0672\n",
      "[10,   264] loss: 0.2040\n",
      "[10,   272] loss: 0.1680\n",
      "[10,   280] loss: 0.1524\n",
      "[10,   288] loss: 0.2819\n",
      "[10,   296] loss: 0.0899\n",
      "[10,   304] loss: 0.1646\n",
      "[10,   312] loss: 0.0944\n",
      "[10,   320] loss: 0.1341\n",
      "[10,   328] loss: 0.1638\n",
      "[10,   336] loss: 0.1998\n",
      "[10,   344] loss: 0.1680\n",
      "[10,   352] loss: 0.1245\n",
      "[10,   360] loss: 0.0931\n",
      "[10,   368] loss: 0.1667\n",
      "[10,   376] loss: 0.1905\n",
      "[10,   384] loss: 0.4130\n",
      "[10,   392] loss: 0.1739\n",
      "[10,   400] loss: 0.1540\n",
      "[10,   408] loss: 0.1938\n",
      "[10,   416] loss: 0.0921\n",
      "[10,   424] loss: 0.1462\n",
      "[10,   432] loss: 0.1065\n",
      "[10,   440] loss: 0.3046\n",
      "[10,   448] loss: 0.0633\n",
      "[10,   456] loss: 0.0650\n",
      "[10,   464] loss: 0.1386\n",
      "[10,   472] loss: 0.2710\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#net = newNetCuda(model=Net3)\n",
    "trainRun(net, epochs=10, lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: \n",
      "Accuracy of  BRCA: 99%\n",
      "Accuracy of  COAD: 98%\n",
      "Accuracy of   GBM: 90%\n",
      "Accuracy of  HNSC: 95%\n",
      "Accuracy of  KIRC: 97%\n",
      "Accuracy of  KIRP: 85%\n",
      "Accuracy of   LGG: 99%\n",
      "Accuracy of  LUAD: 96%\n",
      "Accuracy of  LUSC: 92%\n",
      "Accuracy of  PRAD: 99%\n",
      "Accuracy of  READ:  0%\n",
      "Accuracy of  THCA: 99%\n",
      "Accuracy of  UCEC: 99%\n",
      "Validation set accuracy: \n",
      "Accuracy of  BRCA: 98%\n",
      "Accuracy of  COAD: 91%\n",
      "Accuracy of   GBM: 100%\n",
      "Accuracy of  HNSC: 90%\n",
      "Accuracy of  KIRC: 95%\n",
      "Accuracy of  KIRP: 85%\n",
      "Accuracy of   LGG: 96%\n",
      "Accuracy of  LUAD: 89%\n",
      "Accuracy of  LUSC: 80%\n",
      "Accuracy of  PRAD: 100%\n",
      "Accuracy of  READ:  0%\n",
      "Accuracy of  THCA: 97%\n",
      "Accuracy of  UCEC: 97%\n"
     ]
    }
   ],
   "source": [
    "checkAccuracy(net, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"/seq/tcga/predict_ctype/fpkm13class/tcga13class_convnet1_statedict.pth\")\n",
    "#torch.save(net, \"/seq/tcga/predict_ctype/lung2class/model2net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctypeREADimgs = [d['fpkmvals'] for d in iter(validdata) if d['label'].numpy()[0] == 10]\n",
    "ctypeBRCAimgs = [d['fpkmvals'] for d in iter(validdata) if d['label'].numpy()[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "readimgs = [i.numpy() for i in ctypeREADimgs]\n",
    "brcaimgs = [i.numpy() for i in ctypeBRCAimgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "padzeros = np.zeros((224,270))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow2(img):\n",
    "    #img = img / 2 + 0.5\n",
    "    imgarr = np.array([img[0,:,:], img[0,:,:], img[0,:,:]]).transpose(1,2,0)\n",
    "    plt.imshow(imgarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow3(img):\n",
    "    #img = img / 2 + 0.5\n",
    "    imgarr = np.array([img[0,:,:], padzeros, padzeros]).transpose(1,2,0)\n",
    "    plt.imshow(imgarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAACaCAYAAAAD1vupAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBqODAQAAAAAADEYHAwAAAAAAGOz/A/Zk2zNimbIIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f648e0bc290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first6 = readimgs[:6]\n",
    "\n",
    "f, a = plt.subplots(figsize=(18,18))\n",
    "\n",
    "for (n, i) in enumerate(first6):\n",
    "    plt.gca()\n",
    "    plotnum = 160 + (n + 1)\n",
    "    plt.subplot(plotnum)\n",
    "    imgarr = np.array([i[0,:,:], i[0,:,:], i[0,:,:]]).transpose(1,2,0)\n",
    "    plt.imshow(imgarr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAACaCAYAAAAD1vupAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzRKzswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6579863f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "firstbrca6 = brcaimgs[:6]\n",
    "\n",
    "f, a = plt.subplots(figsize=(18,18))\n",
    "\n",
    "for (n, i) in enumerate(firstbrca6):\n",
    "    plt.gca()\n",
    "    plotnum = 160 + (n + 1)\n",
    "    plt.subplot(plotnum)\n",
    "    imgarr = np.array([i[0,:,:], i[0,:,:], i[0,:,:]]).transpose(1,2,0)\n",
    "    plt.imshow(imgarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAACaCAYAAAAD1vupAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQLT/DzkHsJwV7B49AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64799eab90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "firstbrca6 = brcaimgs[:6]\n",
    "\n",
    "f, a = plt.subplots(figsize=(18,18))\n",
    "\n",
    "for (n, i) in enumerate(firstbrca6):\n",
    "    plt.gca()\n",
    "    plotnum = 160 + (n + 1)\n",
    "    plt.subplot(plotnum)\n",
    "    imgarr = np.array([i[0,:,:], padzeros, padzeros]).transpose(1,2,0)\n",
    "    plt.imshow(imgarr, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fpkm tensors as images to test using image classifier libraries to load and use data\n",
    "t1 = next(iter(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t1['fpkmvals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = '/ssdata/data/tcga/fpkm13class/fpkm_as_jpg'\n",
    "\n",
    "torchvision.utils.save_image(t1['fpkmvals'], imgpath + '/test1.jpg', padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save all tensors in dataset as jpg images with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = GeneExpressionFPKMDatasetN1(\"test_manifest.txt\", \"/ssdata/data/tcga/fpkm13class/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irish/py36torch03/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LUSC'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.class_encoder.inverse_transform(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224, 270])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['fpkmvals'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60480"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 * 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.filename_frame = pd.read_csv((os.path.join(data_dir, samplenames_file)),\n",
    "                                           sep=\"\\s+\", names=[\"filename\",\"label\"],\n",
    "                                          dtype={'filename':'object', 'label':'category'})\n",
    "        self.root_dir = data_dir\n",
    "        #self.labels = pd.get_dummies(self.filename_frame['label']).as_matrix()\n",
    "        self.labels = self.filename_frame['label'].factorize(sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_jpgs(dataset, img_path=None):\n",
    "    \"\"\"Save Dataset instance to jpeg images with class labels as filename prefixes\"\"\"\n",
    "    if img_path is None:\n",
    "        img_path = dataset.root_dir\n",
    "    for (i, e) in enumerate(dataset):\n",
    "        lbl = dataset.class_encoder.inverse_transform(e['label'][0])\n",
    "        jpg_fname = f'{lbl}_{i}.jpg'\n",
    "        torchvision.utils.save_image(e['fpkmvals'], os.path.join(img_path, jpg_fname), padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_jpgs(testdata, \"/ssdata/data/tcga/testjpegs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jpeg files of FPKM values for training and validation sets all in a single folder\n",
    "# NOTE: validation set will be sampled from the training set when jpeg files are loaded into conv_learner\n",
    "# dataset\n",
    "\n",
    "alldata = GeneExpressionFPKMDatasetN1(\"alldata_manifest.txt\",\n",
    "                                      \"/ssdata/data/tcga/fpkm13class/all/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4287"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 13.1 ms, total: 2min 8s\n",
      "Wall time: 58.9 s\n"
     ]
    }
   ],
   "source": [
    "%time save_dataset_to_jpgs(alldata, img_path=\"/ssdata/data/tcga/fpkm13class/all/jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0922c92b36ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testdata' is not defined"
     ]
    }
   ],
   "source": [
    "type(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
